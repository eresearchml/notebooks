{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "#  Author: Angela Chapman\n",
    "#  Date: 8/6/2014\n",
    "#\n",
    "#  This file contains code to accompany the Kaggle tutorial\n",
    "#  \"Deep learning goes to the movies\".  The code in this file\n",
    "#  is for Part 2 of the tutorial and covers Bag of Centroids\n",
    "#  for a Word2Vec model. This code assumes that you have already\n",
    "#  run Word2Vec and saved a model called \"300features_40minwords_10context\"\n",
    "#\n",
    "# *************************************** #\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import copy\n",
    "# Load a pre-trained model\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "\n",
    "\n",
    "# Define a function to create bags of centroids\n",
    "#\n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count\n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running K means\n",
      "Time taken for K Means clustering:  372.5064539909363 seconds.\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "\n",
    "\n",
    "# ****** Run k-means on the word vectors and print a few clusters\n",
    "#\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "print (\"Running K means\")\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3023\n",
      "\n",
      "Cluster 0\n",
      "['tenth']\n",
      "\n",
      "Cluster 1\n",
      "['anonymous', 'opium', 'ss']\n",
      "\n",
      "Cluster 2\n",
      "['shields']\n",
      "\n",
      "Cluster 3\n",
      "['undercurrent', 'illusion', 'aura']\n",
      "\n",
      "Cluster 4\n",
      "['polo', 'superstars', 'brigade']\n",
      "\n",
      "Cluster 5\n",
      "['headstrong', 'saucy', 'gracious', 'perky', 'feisty', 'coy', 'sweetly', 'spunky', 'bitchy', 'sassy', 'plump', 'foxy', 'snobbish', 'hush']\n",
      "\n",
      "Cluster 6\n",
      "['greeted', 'defeated', 'personified', 'teased', 'ridiculed', 'bullied', 'conducted', 'harassed', 'startled', 'terrorized', 'thwarted', 'befriended']\n",
      "\n",
      "Cluster 7\n",
      "['contemplate', 'resolve', 'discuss', 'absorb', 'examine', 'explore', 'observe', 'evaluate']\n",
      "\n",
      "Cluster 8\n",
      "['terrence', 'ron']\n",
      "\n",
      "Cluster 9\n",
      "['smashed', 'blasted', 'dumping', 'someones', 'bashed', 'mangled', 'hammered', 'smashing']\n"
     ]
    }
   ],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number\n",
    "word_centroid_map = dict(zip( model.index2word, idx ))\n",
    "wcm_array = list(word_centroid_map.values())\n",
    "wcm_keys = list(word_centroid_map.keys())\n",
    "#print(wcm_array[0])\n",
    "# Print the first ten clusters\n",
    "for cluster in range(0,10):\n",
    "    #\n",
    "    # Print the cluster number\n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(0,len(wcm_array)):\n",
    "        if( wcm_array[i] == cluster ):\n",
    "            words.append(wcm_keys[i])\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning training reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning test reviews\n",
      "Fitting a random forest to labeled training data...\n",
      "Wrote BagOfCentroids.csv\n"
     ]
    }
   ],
   "source": [
    "# Create clean_train_reviews and clean_test_reviews as we did before\n",
    "#\n",
    "\n",
    "# Read data from files\n",
    "file = '/Users/david/notebooks/'\n",
    "train = pd.read_csv( os.path.join(os.path.dirname(file), 'data', 'labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv(os.path.join(os.path.dirname(file), 'data', 'testData.tsv'), header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "\n",
    "print (\"Cleaning training reviews\")\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( KaggleWord2VecUtility.review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "print (\"Cleaning test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( KaggleWord2VecUtility.review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "\n",
    "# ****** Create bags of centroids\n",
    "#\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews\n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# ****** Fit a random forest and extract predictions\n",
    "#\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print (\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"BagOfCentroids.csv\", index=False, quoting=3)\n",
    "print (\"Wrote BagOfCentroids.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
