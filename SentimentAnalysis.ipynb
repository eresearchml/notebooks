{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pandas, numpy and other packages as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,auc\n",
    "from sklearn.cross_validation import train_test_split,roc_auc_score\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read the data(reviews) which is provided in three different categories - labeled, unlabeled, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeledTrain = pd.read_csv('labeledTrainData.tsv', header=0, quoting=3, delimiter='\\t')\n",
    "testData = pd.read_csv('testData.tsv', header=0, quoting=3, delimiter='\\t')\n",
    "unlabeledTrain = pd.read_csv('unlabeledTrainData.tsv', header=0, quoting=3, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of reviews in each type of dataset in order to check the total number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "50000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(labeledTrain))\n",
    "print(len(unlabeledTrain))\n",
    "print(len(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'sentiment', 'review'], dtype='object')\n",
      "Index(['id', 'review'], dtype='object')\n",
      "Index(['id', 'review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(labeledTrain.columns)\n",
    "print(unlabeledTrain.columns)\n",
    "print(testData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to clean the reviews having noise such as html tags,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_review(review, remove_stopwords=False):\n",
    "    # 1. Remove HTML\n",
    "    text_data = BeautifulSoup(review,'html.parser').get_text()\n",
    "    \n",
    "    # 2. Remove numeric data\n",
    "    text_data = re.sub(\"[^A-Za-z]\",\" \", text_data)\n",
    "    \n",
    "    # 3. Convert letters to lower-case and split the data to form a list of words\n",
    "    words = text_data.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords if true\n",
    "    if remove_stopwords:\n",
    "        s = set(stopwords.words(\"english\"))\n",
    "        words  = [w for w in words if not w in s]\n",
    "        \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the words into word vectors. For this we are going to use the Word2Vec from the gensim package. Word2Vec \n",
    "expects as inputs single sentences. For converting the reviews into a list of sentences, we need to split a review into different sentences for which we will use the 'punkt' tokenizer contained in nltk() for splitting up the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')  ## fo splitting the review into sentences\n",
    "\n",
    "def review_to_sentences(review, tokenizer):\n",
    "    # split the review into different sentences\n",
    "    raw_review = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for raw_sentence in raw_review:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(clean_review(raw_sentence))\n",
    "    \n",
    "    #return the list of sentences where each sentence is a list of words\n",
    "    return(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the reviews in order to use in Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'... ...'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for review in labeledTrain['review']:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    \n",
    "for review in unlabeledTrain['review']:\n",
    "    sentences += review_to_sentences(review,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a word2vec model by providing the parameters of your chocie depending upon the size of your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, workers=4 , min_count=40, size=300, window=5, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'myModel'\n",
    "model.save(model_name)         ## save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape               ##syn0 rows represents the size of vocabulary in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55321699, -0.77782232,  0.82532221, ...,  0.72397226,\n",
       "         0.68414426, -0.35472327],\n",
       "       [ 0.09805842,  0.03103116,  0.53339213, ..., -0.54614192,\n",
       "        -0.55391514, -1.27474058],\n",
       "       [ 0.88128853, -0.43353438,  1.2196058 , ..., -0.67345876,\n",
       "         0.3025381 ,  0.44857651],\n",
       "       ..., \n",
       "       [-0.01545802, -0.09082257,  0.05533485, ...,  0.00173796,\n",
       "        -0.0165602 ,  0.04005823],\n",
       "       [-0.01607578, -0.06936552, -0.08594656, ..., -0.1842301 ,\n",
       "         0.0749278 , -0.14317097],\n",
       "       [ 0.01619974, -0.06358982, -0.08173489, ..., -0.13500381,\n",
       "         0.03976946,  0.06671745]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0                      ## word vectors stored as numpy arrays with each word having dimensions (1x300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each review is of differen length,hence feature vector for each review will be different. In order to handle this problem, we can either attempt 'vector averaging' or 'vector quantization'. It makes sense to make to group similar words together. Hence we can make clusters of word vectors that are somewhat similar. Size of cluster depends on your choice, though small clusters(i.e. clusters with small number of words) are good for making good predictions and it also takes less time to clusters such large number of word vectors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans    ## KMeans will take huge amount of time to cluster so use MiniBatchKMeans\n",
    "\n",
    "word_vectors = model.syn0             ## all the word vectors are now stored in word_vectors\n",
    "n_clusters = word_vectors.shape[0]//5  ## 16490/5 will give a float which will not be accepted as number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.9913558959961\n"
     ]
    }
   ],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100)  \n",
    "idx = kmeans.fit_predict(word_vectors)   ## fit_predict will generate the index for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping of words to the clusters by creating a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_clusters_map = dict(zip(model.index2word, idx))   ##index2word contains indexing for each word in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  :  0\n",
      "['symbol']\n",
      "Cluster  :  1\n",
      "['buscemi', 'martin', 'mcqueen', 'carell']\n",
      "Cluster  :  2\n",
      "['sergeant', 'deputy', 'meanwhile']\n",
      "Cluster  :  3\n",
      "['blame', 'forgive', 'trust', 'tell', 'call']\n",
      "Cluster  :  4\n",
      "['hare']\n",
      "Cluster  :  5\n",
      "['flight', 'landing', 'crash']\n",
      "Cluster  :  6\n",
      "['lost', 'achieved', 'received', 'gained']\n",
      "Cluster  :  7\n",
      "['youth']\n",
      "Cluster  :  8\n",
      "['unlikeable', 'unlikable', 'menacing', 'lovable', 'unsympathetic', 'likeable', 'credible', 'endearing']\n",
      "Cluster  :  9\n",
      "['franz', 'hans']\n"
     ]
    }
   ],
   "source": [
    "##Print out a number of clusters to check the words in a cluster\n",
    "\n",
    "for cluster in range(0,10):\n",
    "    print(\"Cluster  : \", cluster)\n",
    "    words = []\n",
    "    for i in range(len(word_vector_clusters_map.values())):\n",
    "        if(cluster == list(word_vector_clusters_map.values())[i]):\n",
    "           words.append(list(word_vector_clusters_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the reviews into  a bag of clusters. This will give us a numpy array with a fixed size such that for each review we have a fixed number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reviews_clusters_bag(words, word_vector_clusters_map):\n",
    "    ## Number of clusters will be equal to the maximum value in the word_vector_clusters_map\n",
    "    num_clusters  = max(word_vector_clusters_map.values()) + 1\n",
    "    \n",
    "    ## Create a numpy array and initialzie it to zero. Pre-initializing array will help in faster operations\n",
    "    bag_of_clusters = np.zeros(num_clusters, dtype='float32')\n",
    "    \n",
    "    ##Loop over each word in the review, find its cluser and increment the respective numpy array index value\n",
    "    for word in words:\n",
    "        if word in word_vector_clusters_map:\n",
    "            index  = word_vector_clusters_map[word]\n",
    "            bag_of_clusters[index] += 1\n",
    "            \n",
    "    return bag_of_clusters        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the reveiews again by removing the stopwords in order to have low noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_train_reviews = []\n",
    "for review in labeledTrain['review']:\n",
    "    cleaned_train_reviews.append(clean_review(review, remove_stopwords=True))\n",
    "    \n",
    "cleaned_test_reviews = []\n",
    "for review in testData['review']:\n",
    "    cleaned_test_reviews.append(clean_review(review, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make numpy array for training clusters and testing clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_centroids = np.zeros((labeledTrain['review'].size, n_clusters), dtype='float32')\n",
    "test_centroids = np.zeros((testData['review'].size, n_clusters), dtype='float32')\n",
    "\n",
    "index = 0 \n",
    "for review in cleaned_train_reviews:\n",
    "    train_centroids[index] = create_reviews_clusters_bag(review, word_vector_clusters_map)\n",
    "    index = index + 1\n",
    "\n",
    "index = 0\n",
    "for review in cleaned_test_reviews:\n",
    "    test_centroids[index] = create_reviews_clusters_bag(review, word_vector_clusters_map)\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions with an appropriate model like RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_centroids, labeledTrain['sentiment'], test_size=0.2, random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, oob_score=True, n_jobs=-1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = model.predict(test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
