{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as DT\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy as sp\n",
    "import datetime\n",
    "from IPython.core.debugger import Tracer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics.classification import log_loss\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "import operator\n",
    "import itertools\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import utils as ut\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in new data set and get an idea of dimensions\n",
    "## 1) load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coltodatetime(df, col):\n",
    "    df[col] = pd.to_datetime(df[col], infer_datetime_format=True)\n",
    "def splitcol(df, col, char):\n",
    "    df[col] =df[col].fillna('')\n",
    "    r = df[col].apply(lambda x: pd.Series(x.split(char)))\n",
    "    for i in range(0, r.shape[1]):\n",
    "        df[col + str(i)] = r[i]\n",
    "    #del df[col]\n",
    "def combinedatentime(df, datecol, timecol, newcol):\n",
    "    df[newcol] = df.apply(lambda row: datetime.datetime.combine(row[datecol].date(), row[timecol].time()), axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape\n",
      "(7546184, 5)\n",
      "(991891, 8)\n",
      "Columns\n",
      "account_num                 int64\n",
      "date               datetime64[ns]\n",
      "ledger_balance            float64\n",
      "cleared_balance           float64\n",
      "month                       int64\n",
      "dtype: object\n",
      "account_num                    int64\n",
      "post_date             datetime64[ns]\n",
      "value_date            datetime64[ns]\n",
      "description                   object\n",
      "amount                       float64\n",
      "country_code                  object\n",
      "overseas_indicator            object\n",
      "BACS_user_num                  int64\n",
      "dtype: object\n",
      "Samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_num</th>\n",
       "      <th>date</th>\n",
       "      <th>ledger_balance</th>\n",
       "      <th>cleared_balance</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13320029457242</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>181.5</td>\n",
       "      <td>181.5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13320029457242</td>\n",
       "      <td>2015-12-02</td>\n",
       "      <td>181.5</td>\n",
       "      <td>181.5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13320029457242</td>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>181.5</td>\n",
       "      <td>181.5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13320029457242</td>\n",
       "      <td>2015-12-04</td>\n",
       "      <td>181.5</td>\n",
       "      <td>181.5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13320029457242</td>\n",
       "      <td>2015-12-05</td>\n",
       "      <td>81.5</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      account_num       date  ledger_balance  cleared_balance  month\n",
       "0  13320029457242 2015-12-01           181.5            181.5     12\n",
       "1  13320029457242 2015-12-02           181.5            181.5     12\n",
       "2  13320029457242 2015-12-03           181.5            181.5     12\n",
       "3  13320029457242 2015-12-04           181.5            181.5     12\n",
       "4  13320029457242 2015-12-05            81.5             81.5     12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_num</th>\n",
       "      <th>post_date</th>\n",
       "      <th>value_date</th>\n",
       "      <th>description</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12207455809793</td>\n",
       "      <td>2015-12-15</td>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>PAYPAL</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12207455809793</td>\n",
       "      <td>2015-12-24</td>\n",
       "      <td>2015-12-29</td>\n",
       "      <td>HSBC</td>\n",
       "      <td>276.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12207455809793</td>\n",
       "      <td>2015-12-30</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>PAYPAL</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12207455809793</td>\n",
       "      <td>2015-12-08</td>\n",
       "      <td>2015-12-09</td>\n",
       "      <td>PAYPAL</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12207455809793</td>\n",
       "      <td>2015-12-22</td>\n",
       "      <td>2015-12-23</td>\n",
       "      <td>PAYPAL</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12207455809793</td>\n",
       "      <td>2015-12-30</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>PAYPAL</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12207455809793</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>SANTANDER</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12207455809793</td>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>2015-12-04</td>\n",
       "      <td>PAYPAL</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      account_num  post_date value_date description  amount\n",
       "0  12207455809793 2015-12-15 2015-12-16      PAYPAL     8.0\n",
       "1  12207455809793 2015-12-24 2015-12-29        HSBC   276.0\n",
       "2  12207455809793 2015-12-30 2015-12-31      PAYPAL    26.0\n",
       "3  12207455809793 2015-12-08 2015-12-09      PAYPAL    22.5\n",
       "4  12207455809793 2015-12-22 2015-12-23      PAYPAL    17.5\n",
       "5  12207455809793 2015-12-30 2015-12-31      PAYPAL    16.5\n",
       "6  12207455809793 2015-12-31 2016-01-04   SANTANDER    80.0\n",
       "7  12207455809793 2015-12-03 2015-12-04      PAYPAL    13.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>overseas_indicator</th>\n",
       "      <th>BACS_user_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GBP</td>\n",
       "      <td>N</td>\n",
       "      <td>679848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GBP</td>\n",
       "      <td>N</td>\n",
       "      <td>599938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBP</td>\n",
       "      <td>N</td>\n",
       "      <td>679848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBP</td>\n",
       "      <td>N</td>\n",
       "      <td>679848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GBP</td>\n",
       "      <td>N</td>\n",
       "      <td>679848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GBP</td>\n",
       "      <td>N</td>\n",
       "      <td>679848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GBP</td>\n",
       "      <td>N</td>\n",
       "      <td>880141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GBP</td>\n",
       "      <td>N</td>\n",
       "      <td>679848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code overseas_indicator  BACS_user_num\n",
       "0          GBP                  N         679848\n",
       "1          GBP                  N         599938\n",
       "2          GBP                  N         679848\n",
       "3          GBP                  N         679848\n",
       "4          GBP                  N         679848\n",
       "5          GBP                  N         679848\n",
       "6          GBP                  N         880141\n",
       "7          GBP                  N         679848"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#f1 = \"C:/Users/lloyd/data/LFB incident data 1 Jan 2009 to 31 Aug 2015/LFB incident data 1 Jan 2009 to 31 Dec 2011.csv\"\n",
    "f1 = \"/Users/david/data/historical_balance.csv\"\n",
    "f2 = \"/Users/david/data/historical_direct_debits.csv\"\n",
    "\n",
    "df1 = pd.read_csv(f1, header=0, nrows = 10000000)\n",
    "df2 = pd.read_csv(f2, header=0, nrows = 10000000)\n",
    "coltodatetime(df1, \"date\")\n",
    "coltodatetime(df2, \"post_date\")\n",
    "coltodatetime(df2, \"value_date\")\n",
    "df1['month'] = df1[\"date\"].apply(lambda x : x.month)\n",
    "\n",
    "print(\"Dataframe shape\")\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "\n",
    "print(\"Columns\")\n",
    "print(df1.dtypes)\n",
    "print(df2.dtypes)\n",
    "print(\"Samples\")\n",
    "ut.dispdf(df1,5)\n",
    "ut.dispdf(df2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) figure out primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isunique(df, col):\n",
    "    return  len(df[col]) ==  len(set(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"IsUnique \", isunique(df, \"IncidentNumber\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) figure out what types of columns you have, numerical, categorical and nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitcoltypes(df):\n",
    "    numcols = []\n",
    "    datcols = []\n",
    "    catcols = []\n",
    "    for i in range(len(df.columns)):\n",
    "        if(df.dtypes[i] == \"object\"):\n",
    "            catcols.append(df.columns[i])\n",
    "        elif(df.dtypes[i] == \"datetime64[ns]\"):\n",
    "            datcols.append(df.columns[i])\n",
    "        else:\n",
    "            numcols.append(df.columns[i])\n",
    "    return numcols, datcols, catcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-95c5e0b16983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitcoltypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Numerical Columns\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Datetime Columns\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "numcols, datcols, catcols = splitcoltypes(df)\n",
    "print(\"Numerical Columns\", len(numcols))\n",
    "print(numcols, \"\\n\")\n",
    "print(\"Datetime Columns\", len(datcols))\n",
    "print(datcols, \"\\n\")\n",
    "print(\"Categorical Columns\", len(catcols))\n",
    "print(catcols, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) deal with numerical\n",
    "- 4a) for each column, min, max, count, missing, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mad_based_outlier(points, thresh=3.5):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh\n",
    "\n",
    "def colstats(df, c):\n",
    "    vals = df[c]\n",
    "    count = len(vals)\n",
    "    min = np.min(vals)\n",
    "    max = np.max(vals)\n",
    "    per_25 = np.percentile(vals, 25)\n",
    "    mean = np.mean(vals)\n",
    "    mode = scipy.stats.mode(vals)\n",
    "    per_75 = np.percentile(vals, 75)\n",
    "    std = np.std(vals)\n",
    "    #expected 3\n",
    "    kurt = sp.stats.kurtosis(vals)\n",
    "    skew = sp.stats.skew(vals)\n",
    "    nans = vals.isnull().sum()\n",
    "    per_nans = nans/count * 100\n",
    "    out_num = len(df[c][mad_based_outlier(df[c].values)])\n",
    "    out_per = out_num/count * 100\n",
    "\n",
    "    d = {\"count\": count, \"min\": min, \"max\": max, \"per_25\": per_25, \"mean\": mean,\n",
    "         \"per_75\": per_75, \"std\": std, \"kurt\": kurt, \"skew\": skew, \"nans\": nans, \n",
    "         \"nans_per\": per_nans, \"out_num\": out_num, \"out_per\": out_per, \"mode\":mode\n",
    "        }\n",
    "    \n",
    "    st = pd.DataFrame.from_dict(d, orient='index')\n",
    "    st.rename(columns={0: c }, inplace=True)\n",
    "    st.sort_index(inplace=True)\n",
    "    return st\n",
    "\n",
    "def coldatetimestats(df, c):\n",
    "    vals = df[c]\n",
    "    count = len(vals)\n",
    "    min = np.min(vals)\n",
    "    max = np.max(vals)\n",
    "    nans = vals.isnull().sum()\n",
    "    per_nans = nans/count * 100\n",
    "    \n",
    "\n",
    "    d = {\"count\": count, \"min\": min, \"max\": max,\"nans\": nans, \n",
    "         \"nans_per\": per_nans, \n",
    "        }\n",
    "    \n",
    "    st = pd.DataFrame.from_dict(d, orient='index')\n",
    "    st.rename(columns={0: c }, inplace=True)\n",
    "    st.sort_index(inplace=True)\n",
    "    return st\n",
    "\n",
    "def catstats(df, c):\n",
    "    vc = df[c].value_counts()\n",
    "    count = len(vc)\n",
    "\n",
    "    d = {\"catcount\": count, \n",
    "        }\n",
    "    \n",
    "    st = pd.DataFrame.from_dict(d, orient='index')\n",
    "    st.rename(columns={0: c }, inplace=True)\n",
    "    st.sort_index(inplace=True)\n",
    "    return st\n",
    "\n",
    "\n",
    "def removenans(df, c):\n",
    "    return df.dropna(subset=[c])\n",
    "\n",
    "def dataframestats(df):\n",
    "    stats = pd.DataFrame()\n",
    "    for c in df.columns:\n",
    "        if(not df[c].dtype == \"object\" and not df[c].dtype == \"datetime64[ns]\"):\n",
    "            st = colstats(df, c)\n",
    "            stats[c] = st[c]\n",
    "    return stats\n",
    "\n",
    "def dataframedatetimestats(df):\n",
    "    stats = pd.DataFrame()\n",
    "    for c in df.columns:\n",
    "        if(df[c].dtype == \"datetime64[ns]\"):\n",
    "            st = coldatetimestats(df, c)\n",
    "            stats[c] = st[c]\n",
    "    return stats\n",
    "\n",
    "\n",
    "def dataframecatstats(df_local):\n",
    "    stats = pd.DataFrame()\n",
    "    for c in df_local.columns:\n",
    "        if(not df_local[c].dtype == 'float64' and not df_local[c].dtype == 'int64'):\n",
    "            print(df_local.shape)\n",
    "            st = catstats(df_local, c)\n",
    "            stats[c] = st[c]\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4b) shapes of distribution\n",
    "- 4c) fill in missing values/drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dist functions\n",
    "def showdist(df, c, threshold=3.5):\n",
    "    ut.figurefullwidth()\n",
    "    nans = df[c].isnull().sum()\n",
    "    vals = df[c].dropna().values\n",
    "    ut.subplottitle(1, \"Nans\", w=6)\n",
    "    plt.bar([1, 2], [len(df[c])-nans,nans], tick_label = [\"Ok\", \"NaN\"])\n",
    "    ut.subplottitle(2, \"Hist\", w=6)\n",
    "    plt.hist(vals, bins=20)\n",
    "    ut.subplottitle(3, \"Plot\", w=6)\n",
    "    plt.plot(vals, 'o')\n",
    "    ut.subplottitle(4, \"Box\", w=6)\n",
    "    sns.boxplot(vals)\n",
    "    outliers = mad_based_outlier(df[c].values)\n",
    "    outpoints = df[c][outliers]\n",
    "    ut.subplottitle(5, \"Distplot\", w=6)\n",
    "    sns.distplot(df[c], bins=20, hist=False)\n",
    "    ut.subplottitle(6, \"Outliers\", w=6)\n",
    "    plt.plot(outpoints, 'ro')\n",
    "    out_per = (len(outpoints)/df.shape[0])*100\n",
    "    nan_per = (nans/df.shape[0])*100\n",
    "    print(c,df.shape,\"Ouliers\", len(outpoints), \"{:1.2f}\".format(out_per),\"%\",\"Nans\",nans, \"{:1.2f}\".format(nan_per),\"%\",\"\\n\")\n",
    "    plt.show()\n",
    "    \n",
    "def examineonenumcol(df, c):\n",
    "    showdist(df, c)\n",
    "    #df2 = removeoutliers(df, c)\n",
    "    #chg = df2.shape[0] - df.shape[0]\n",
    "    #chg_per = chg / df.shape[0] * 100.0\n",
    "    #print(c, \"After\",df2.shape,chg, \"{:10.2f}\".format(chg_per) + \"%\", \"\\n\")\n",
    "    #showdist(df2, c)\n",
    "\n",
    "def removeoutliers(df, c, threshold=3.5):\n",
    "    outliers = mad_based_outlier(df[c].values)\n",
    "    outpoints = df[c][outliers]\n",
    "    return df.drop(outpoints.index)\n",
    "\n",
    "def removeoutliersfromcols(df, cols, threshold=3.5):\n",
    "    print(\"#===============================================\")\n",
    "    print(\"# Remove outliers from\", cols)\n",
    "    print(\"#===============================================\")\n",
    "\n",
    "    df2 = df.copy(deep=True)\n",
    "\n",
    "    for c in cols:\n",
    "        if(c in df2.columns):\n",
    "            df2 = removeoutliers(df2, c)\n",
    "    return df2\n",
    "\n",
    "def removecols(df, cols):\n",
    "    print(\"#===============================================\")\n",
    "    print(\"# Remove cols \", cols)\n",
    "    print(\"#===============================================\")\n",
    "    df2 = df.copy(deep=True)\n",
    "\n",
    "    for c in cols:\n",
    "        if(c in df2.columns):\n",
    "            df2 = df2.drop(c, axis=1)\n",
    "    return df2\n",
    "def removenansfromcols(df, cols):\n",
    "    print(\"#===============================================\")\n",
    "    print(\"# Remove nan from cols \", cols)\n",
    "    print(\"#===============================================\")\n",
    "    df2 = df.copy(deep=True)\n",
    "\n",
    "    for c in cols:\n",
    "        if(c in df2.columns):\n",
    "            val_list = df2[df2[c].apply(lambda x: np.isnan(x))]\n",
    "            print(len(val_list))\n",
    "            df2 =df2.drop(val_list.index)\n",
    "    return df2\n",
    "def makenormalfromexp(df, cols):\n",
    "    print(\"#===============================================\")\n",
    "    print(\"# make normal from exp\", cols)\n",
    "    print(\"#===============================================\")\n",
    "    df2 = df.copy(deep=True)\n",
    "\n",
    "    for c in cols:\n",
    "        if(c in df2.columns):\n",
    "            df2[c] = df2[c].apply(lambda x: 1/x)\n",
    "    return df2\n",
    "def makenormalfromlog(df, cols):\n",
    "    print(\"#===============================================\")\n",
    "    print(\"# make normal from lognormal\", cols)\n",
    "    print(\"#===============================================\")\n",
    "    df2 = df.copy(deep=True)\n",
    "\n",
    "    for c in cols:\n",
    "        if(c in df2.columns):\n",
    "            df2[c] = df2[c].apply(lambda x: np.log(x))\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main area for adding / removing columns and transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dispcatdata(df, c):\n",
    "    vc = df[c].value_counts()\n",
    "    print(c, \" counttypes \",vc.shape[0],  \"\\n\")\n",
    "    if(vc.shape[0] > 100):\n",
    "        print(\"Data is large!!!!!!!!!!!! \" , vc.shape[0])\n",
    "    cumsum = np.cumsum(vc.values)\n",
    "    cumsum = cumsum/len(df[c])\n",
    "    if False:\n",
    "        ut.subplottitle(1, \"CumSum\")\n",
    "        plt.plot(cumsum)\n",
    "    #print(vc)\n",
    "    ut.figurefullwidth()\n",
    "    vc[:100].plot(kind='bar')\n",
    "    plt.show()\n",
    "    pro = 0;\n",
    "    for n in vc.values:\n",
    "        pro\n",
    "    \n",
    "def sigvals(df, c, threshold=0.8, num=0):\n",
    "    print(df.columns, c)\n",
    "    vc = df[c].value_counts()\n",
    "    names = vc.index.tolist()\n",
    "   \n",
    "    count = len(df[c])\n",
    "    cutoff = (int)(count * threshold)\n",
    "    acc = 0\n",
    "    num_acc = 0;\n",
    "    ret = []\n",
    "    others = []\n",
    "    for i in range(vc.shape[0]):\n",
    "        #print(names[i])\n",
    "        if acc < cutoff and num_acc < num -1:\n",
    "            ret.append(names[i])\n",
    "        else:\n",
    "            others.append(names[i])\n",
    "        acc += vc[i]\n",
    "        num_acc += 1\n",
    "    return ret, others\n",
    "\n",
    "\n",
    "def filteronsig(df, cols, threshold=0.8, num=0):\n",
    "    df2 = df.copy(deep=True)\n",
    "    for c in cols:\n",
    "        #print(c)\n",
    "        sig, others = sigvals(df2, c, threshold=threshold, num=num)\n",
    "        df2[c] = df2[c].apply(lambda x: x if x in sig else \"other\")\n",
    "    return df2\n",
    "\n",
    "def examineonecatcol(df_local, c, threshold):\n",
    "    df_temp = filteronsig(df_local, [c], threshold=threshold)\n",
    "    diff = df_local.shape[0] - df_temp.shape[0]\n",
    "    print(df_temp.shape, diff, \"{:1.2f}\".format(diff/df_local.shape[0]*100), \"% elements in bottom \", \"{:1.2f}\".format((1-threshold)*100), \"% of categories\")\n",
    "    dispcatdata(df_local, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = dataframedatetimestats(df1)\n",
    "ut.dispdf(stats, 5, num=20)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "gb1 = df2.groupby(['account_num'])['account_num'].count()\n",
    "gb1[12207455809793]\n",
    "\n",
    "s1 = set(df1[\"account_num\"])\n",
    "df3 = pd.DataFrame(list(s1), columns=[\"account_num\"])\n",
    "df3[\"dd_num\"] = df3[\"account_num\"].apply(lambda a: gb1.get(a, 0))\n",
    "df3\n",
    "#df1[\"dd_num\"] = =\n",
    "\n",
    "stats = dataframestats(df3)\n",
    "ut.dispdf(stats, 5, num=20)\n",
    "examineonenumcol(df3, \"dd_num\")\n",
    "#s.get('f', np.nan)\n",
    "#pd.get_option('display.max_rows')\n",
    "dft1 = df1[df1[\"account_num\"]==16232155578371]\n",
    "dft1\n",
    "dft2 = df2[df2[\"account_num\"]==16232155578371]\n",
    "dft2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehotencodings = {}\n",
    "def onehotencode(df, cols):\n",
    "    for c in cols:\n",
    "        if(c in df.columns):\n",
    "            pre_cols = df.columns\n",
    "            temp_col = df[c]\n",
    "            df = pd.get_dummies(df, columns=[c])\n",
    "            df[c] = temp_col\n",
    "            post_cols = df.columns\n",
    "            diff_cols = list(set(post_cols) - set(pre_cols))\n",
    "            onehotencodings[c] = diff_cols\n",
    "    return df\n",
    "   \n",
    "#dummyencodings = {}\n",
    "#def dummyencode(df2, col):\n",
    "#    if(df2[c].dtype == \"object\"):\n",
    "#        le = LabelEncoder()\n",
    "#        df2[c] = df2[c].apply(lambda x: 'NaN' if pd.isnull(x) else x)\n",
    "#        df2[c] = le.fit_transform(df2[c])\n",
    "#        encodings[c] = le\n",
    "#    return df\n",
    "\n",
    "def encodecols(df, catcols, dummy=False):\n",
    "    df2 = df.copy(deep=True)\n",
    "#    if(dummy):\n",
    "#        for c in catcols:\n",
    "#            if(c in df2.columns):\n",
    "#                df2 = dummyencode(df2, c)\n",
    "#    else:\n",
    "    df2 = onehotencode(df2, catcols)\n",
    "    return df2\n",
    "\n",
    "def printencodings(catcols):\n",
    "    tenc1 = pd.DataFrame()\n",
    "    for c in catcols:\n",
    "        if(c in onehotencodings):\n",
    "            classes = onehotencodings[c]\n",
    "            tenc = pd.DataFrame(classes, index = range(len(classes)))\n",
    "            tenc[c] = tenc[0]\n",
    "            del tenc[0]\n",
    "            ut.dispdf(tenc, num=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_temp = df\n",
    "print(\"Numerical columns\")\n",
    "print (numcols)\n",
    "print(\"Categorical columns\")\n",
    "print (catcols)\n",
    "outcols = []\n",
    "remcols = []\n",
    "nancols =  []\n",
    "logcols = []\n",
    "filtcols = []\n",
    "    \n",
    "if(False):\n",
    "    outcols = []\n",
    "    remcols = []\n",
    "    nancols =  []\n",
    "    logcols = []\n",
    "\n",
    "# After examination of dataset\n",
    "#===============================================\n",
    "# 1) Remove certain value, Nans \n",
    "#===============================================\n",
    "df_temp = removenansfromcols(df_temp,nancols)\n",
    "#===============================================\n",
    "# 2) make cols normal through ln\n",
    "#===============================================\n",
    "df_temp = makenormalfromlog(df_temp, logcols)\n",
    "#===============================================\n",
    "# 3) Remove outliers on cols\n",
    "#===============================================\n",
    "df_temp = removeoutliersfromcols(df_temp, outcols)\n",
    "#===============================================\n",
    "# 4) Remove cols that don't matter\n",
    "#===============================================\n",
    "df_temp = removecols(df_temp, remcols)\n",
    "#===============================================\n",
    "# 5) Filter cols with large cat number\n",
    "#===============================================\n",
    "df_temp = filteronsig(df_temp,filtcols, num=20)\n",
    " \n",
    "#===============================================\n",
    "# Examine the dataset\n",
    "#===============================================\n",
    "ignorecols = []\n",
    "#ignorecols = ['IncidentNumber', 'Easting_m','Northing_m','Easting_rounded','Northing_rounded']\n",
    "len(numcols)\n",
    "stats = dataframestats(df_temp)\n",
    "ut.dispdf(stats, 5, num=20)\n",
    "\n",
    "\n",
    "#Skip/Include examine of numericals\n",
    "if(True):\n",
    "    for c in numcols:\n",
    "        if(c not in ignorecols and c in df_temp.columns):\n",
    "            examineonenumcol(df_temp, c)\n",
    "        \n",
    "threshold = 0.8\n",
    "print(df_temp.shape)\n",
    "cstats = dataframecatstats(df_temp)\n",
    "ut.dispdf(cstats, 5, num=20)\n",
    "if(True):\n",
    "    for c in catcols:\n",
    "        if(c not in ignorecols and c in df_temp.columns):\n",
    "            examineonecatcol(df_temp, c, threshold)\n",
    "\n",
    "#===============================================\n",
    "# 6) ASSIGN it back to df - when you are happy with it!!!!!\n",
    "#===============================================\n",
    "if(True):\n",
    "    print(\"#===============================================\")\n",
    "    print(\"# ONE HOT ENCODING \")\n",
    "    print(\"#===============================================\")\n",
    "    #do the one hot encoding\n",
    "    print(catcols)\n",
    "    print(df_temp.shape)\n",
    "    df_temp = encodecols(df_temp, catcols)\n",
    "\n",
    "    df_orig = df\n",
    "    df = df_temp\n",
    "    df = df.reset_index(drop=True)\n",
    "    numcols, datcols, catcols = splitcoltypes(df)\n",
    "    print(\"#===============================================\")\n",
    "    print(\"# COMPLETE DATA CLEAN FOR df \")\n",
    "    print(\"#===============================================\")\n",
    "\n",
    "#print(\"#===============================================\")\n",
    "#print(\"# AFTER TRANSFORM\")\n",
    "#print(\"#===============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) deal with categorical\n",
    "- 5a) for each col count values\n",
    "- 5b) fill in missing values/drop them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) nlp -> look but dont touch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Finally remove the columns that dont' matter to get final dataset we can work on clealy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Nice clean data set we can do something to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a) do a big old linear regression map and them iso map it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     38
    ],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_matrix(cm, classesx, classesy,\n",
    "                          normalize=True,\n",
    "                          title='matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          axis=0):\n",
    "   \n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marksx = np.arange(len(classesx))\n",
    "    plt.xticks(tick_marksx, classesx, rotation=90)\n",
    "    tick_marksy = np.arange(len(classesy))\n",
    "    plt.yticks(tick_marksy, classesy)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid(False) \n",
    "    #for axi in (ax.xaxis, ax.yaxis):\n",
    "    #    for tic in axi.get_major_ticks():\n",
    "    #        tic.tick1On = tic.tick2On = True\n",
    "    #        tic.label1On = tic.label2On = False\n",
    "    orig = cm\n",
    "    if normalize:\n",
    "        if axis == 1:\n",
    "            cm = cm.astype('float') / cm.sum(axis=axis)[:, np.newaxis]\n",
    "        else:\n",
    "            cm = cm.astype('float') / cm.sum(axis=axis)\n",
    "    thresh = (cm.max() - cm.min()) * 0.5 + cm.min()\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        s = \"{:1.2f}\".format(cm[i,j])\n",
    "        #s = \"{:1.4f}\".format(cm[i,j]) + \"(\"+str(orig[i,j])+\")\"\n",
    "        plt.text(j, i, s, horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('Y label')\n",
    "    plt.xlabel('X label')\n",
    "\n",
    "def onelr(x1, y1):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x1, y1)\n",
    "    \n",
    "    return lr.score(x1, y1)\n",
    "\n",
    "def plotcorrel(df_temp, colsx, colsy):\n",
    "    num_coef = np.empty((len(colsx),len(colsy),))\n",
    "    num_coef[:] = np.NAN\n",
    "    x_labels = []\n",
    "    y_labels = []\n",
    "    for i in range(len(colsx)):\n",
    "        c_i = colsx[i]\n",
    "        if(c_i in df_temp.columns):\n",
    "            df_temp = df_temp.dropna(subset=[c_i])\n",
    "            y_labels.append(c_i + \"(\"+str(df_temp.shape[0])+\")\")\n",
    "            #print(c_i, df_temp.shape)\n",
    "            for j in range(len(colsy)):\n",
    "                c_j = colsy[j]\n",
    "                if len(x_labels)  < len(colsy):\n",
    "                     x_labels.append(c_j + \"(\"+str(df_temp.shape[0])+\")\")\n",
    "                if(c_j in df_temp.columns):\n",
    "                    df_temp = df_temp.dropna(subset=[c_j])\n",
    "                    vals_i = df_temp[c_i].values.reshape(-1,1)\n",
    "                    vals_j = df_temp[c_j].values.reshape(-1,1)\n",
    "                    if(np.isnan(vals_j).sum() == 0):\n",
    "                        #r = onelr(vals_i, vals_j)\n",
    "                        pcor = sp.stats.pearsonr(vals_i, vals_j)\n",
    "                        #print(pcor)\n",
    "                        num_coef[i, j] = pcor[0][0]\n",
    "    #print(num_coef)\n",
    "    ut.figurefullwidth()\n",
    "    plot_matrix(num_coef, x_labels, y_labels, normalize=False, title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "numcolsexOH = numcols\n",
    "for key in onehotencodings.keys():\n",
    "    numcolsexOH = list(set(numcolsexOH) - set(onehotencodings[key]))\n",
    "print(numcolsexOH)\n",
    "plotcorrel(df, numcolsexOH, numcolsexOH)\n",
    "plt.figure()\n",
    "#plt.plot(df[\"NumStationsWithPumpsAttending\"], df[\"NumPumpsAttending\"], 'o')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " \n",
    "def plot_catmatrix(cm, x_labels, y_labels,\n",
    "                          normalize=False,\n",
    "                          title='matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          y_title=\"\", x_title=\"\",\n",
    "                          axis=0):\n",
    "   \n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    #tick_marks = np.arange(len(x_labels))\n",
    "    plt.xticks(np.arange(len(x_labels)), x_labels, rotation=90)\n",
    "    plt.yticks(np.arange(len(y_labels)), y_labels)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid(False) \n",
    "    #for axi in (ax.xaxis, ax.yaxis):\n",
    "    #    for tic in axi.get_major_ticks():\n",
    "    #        tic.tick1On = tic.tick2On = True\n",
    "    #        tic.label1On = tic.label2On = False\n",
    "    orig = cm\n",
    "    if normalize:\n",
    "        if axis == 1:\n",
    "            cm = cm.astype('float') / cm.sum(axis=axis)[:, np.newaxis]\n",
    "        else:\n",
    "            cm = cm.astype('float') / cm.sum(axis=axis)\n",
    "    thresh = (cm.max() - cm.min()) * 0.5 + cm.min()\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        v = cm[i,j]\n",
    "        if(np.isnan(v)):\n",
    "            s = '-'\n",
    "        else:\n",
    "            s = \"{:1.2f}\".format(v)\n",
    "        #s = \"{:1.4f}\".format(cm[i,j]) + \"(\"+str(orig[i,j])+\")\"\n",
    "        plt.text(j, i, s, horizontalalignment=\"center\",fontsize=8,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(y_title)\n",
    "    plt.xlabel(x_title)\n",
    "    plt.show()\n",
    "\n",
    "def vc(df, c1, num=30):\n",
    "    vc1 = df[c1].value_counts()\n",
    "    n1 = vc1.index.tolist()\n",
    "    \n",
    "    if(len(n1) > num):\n",
    "        df_temp = filteronsig(df, [c1], num=num)\n",
    "        vc1 = df_temp[c1].value_counts()\n",
    "        n1 = vc1.index.tolist()\n",
    "    return vc1, n1\n",
    "\n",
    "\n",
    "            \n",
    "def plotcols(df, c1, c2):\n",
    "    print(c1, c2)\n",
    "    if(c1 in df.columns):\n",
    "        vc1, n1 = vc(df, c1)\n",
    "    else:\n",
    "        return\n",
    "    if(c2 in df.columns):\n",
    "        vc2, n2 = vc(df, c2)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    counts = np.empty((len(n1),len(n2),))\n",
    "    counts[:] = np.NAN\n",
    "    #print(vc2)\n",
    "    for i in range(len(n1)):\n",
    "        for j in range(len(n2)):\n",
    "            if(c1 in df.columns and c2 in df.columns):\n",
    "                c = len(df[(df[c1]==n1[i]) & (df[c2]==n2[j])])\n",
    "                #print(c)\n",
    "                counts[i,j] = c\n",
    "    #print(counts)\n",
    "    plot_catmatrix(np.transpose(counts), n1, n2, normalize=True, y_title=c2, x_title=c1)\n",
    "    #x = LabelEncoder().fit_transform(df[c1])\n",
    "    #y = LabelEncoder().fit_transform(df[c2])\n",
    "    #plt.plot(x, y, 'o')\n",
    "    #plt.show()\n",
    "len(catcols)\n",
    "#print(df.head())\n",
    "print(catcols)\n",
    "for c in catcols:\n",
    "    plotcols(df, catcols[0], c)\n",
    "#plotcols(df, catcols[0], catcols[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in catcols:\n",
    "    ohlist = onehotencodings[i]\n",
    "    print(i)\n",
    "    #print(ohlist)\n",
    "    #print(df_temp.shape)\n",
    "    ut.figurefullwidth()\n",
    "    plotcorrel(df_temp, ohlist, [\"FirstPumpArriving_AttendanceTime\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot categories vs 1 numberical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def boxplotcats(df, c2, catcols=catcols):\n",
    "    for c in catcols:\n",
    "        if(len(onehotencodings[c]) < 100):\n",
    "            #df_sub = inverseonehotencode(df, c,onehotencodings[c])\n",
    "            _ = sns.boxplot(x=c, y=c2, data=df)\n",
    "            _ = plt.xticks(rotation=90)\n",
    "            plt.show()\n",
    "\n",
    "def inverseonehotencode(df, col, cols):\n",
    "    df2 = df.copy(deep=True)\n",
    "    x = df2[cols].stack()\n",
    "    s = pd.Series(pd.Categorical(x[x!=0].index.get_level_values(1)))\n",
    "    df2[col] = s\n",
    "    df2 = df2.drop(cols, axis=1)\n",
    "    #print(s)\n",
    "    return df2\n",
    "\n",
    "#print(df.head())\n",
    "#print(inverseonehotencode(df, catcols[0],onehotencodings[catcols[0]]).head())\n",
    "#shall I do a clustering on the pump arrival times? \n",
    "#arrival times vs station?\n",
    "c2 = \"FirstPumpArriving_AttendanceTime\"\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd, MultiComparison\n",
    "\n",
    "#t test means of data across category\n",
    "for c in catcols:\n",
    "    try:\n",
    "        print(c2, c)\n",
    "        p_vals = []\n",
    "        labels = []\n",
    "        sub_values = []\n",
    "        for c_sub1 in onehotencodings[c]:\n",
    "            sub_values.append(list(df[df[c_sub1] == 1][c2].values))\n",
    "        #print(sub_values)\n",
    "        #print(sub_values)\n",
    "        #ANOVA test 1 way\n",
    "        stat, pval = sp.stats.f_oneway(*sub_values)\n",
    "        print(\"ANOVA\", stat, pval)\n",
    "        #df_sub = inverseonehotencode(df, c, onehotencodings[c])\n",
    "        #print(df_sub[c].head())\n",
    "        #print(df_sub[c2].head())\n",
    "        mod = MultiComparison(df[c2], df[c])\n",
    "        tukey = mod.tukeyhsd()\n",
    "        print(mod.groupsunique)\n",
    "        #print()\n",
    "        #print(tukey.meandiffs)\n",
    "        print (tukey)\n",
    "        combs = list(itertools.combinations(mod.groupsunique, 2))\n",
    "        #print(len(combs))\n",
    "        ave_means = [];\n",
    "        for d in mod.groupsunique:\n",
    "            #print(d)\n",
    "            sum_means = 0;\n",
    "            count_means = 0;\n",
    "            for i in range(len(combs)):\n",
    "                #print(d, i)\n",
    "                a, b = combs[i]\n",
    "                if(a == d or b == d):\n",
    "                    sum_means += tukey.meandiffs[i]\n",
    "                    count_means += 1\n",
    "            ave_means.append(sum_means/count_means)\n",
    "        #print(ave_means, np.arange(len(mod.groupsunique)))\n",
    "        _ = plt.bar(np.arange(len(mod.groupsunique)), ave_means, align='center')\n",
    "        _ = plt.xticks(np.arange(len(mod.groupsunique)), mod.groupsunique)\n",
    "        _ = plt.xticks(rotation=90)\n",
    "        _ = plt.show()\n",
    "        boxplotcats(df, c2, catcols=[c])\n",
    "       \n",
    "    except Exception as e: \n",
    "        print (str(e))\n",
    "    #for c_sub1 in onehotencodings[c]:\n",
    "    #    sub1 = df[df[c_sub1] == 1][c2].values\n",
    "    #    sub1_not = df[df[c_sub1] == 0][c2].values\n",
    "    #    #Welch's t-test\n",
    "    #    _, p_val = sp.stats.ttest_ind(sub1, sub1_not, equal_var = False)\n",
    "    #    sig = p_val < 0.05\n",
    "    #    p_vals.append(p_val)\n",
    "    #    labels.append(c_sub1)\n",
    "    #    print(c_sub1, \"{:1.4f}\".format(p_val), \"Sig=\", sig, \"sub1mean=\", sub1.mean(), \"sub2mean=\", sub1_not.mean())\n",
    "    #plot_catmatrix(np.matrix([p_vals]), labels, [c2], normalize=False, y_title=c2, x_title=c)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2 numerical - not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot2numerical(df_temp, c1, c2):\n",
    "    x = df_temp[c1]\n",
    "    y = df_temp[c2]\n",
    "    # fit with np.polyfit\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y, '.')\n",
    "    plt.plot(x, m*x + b, '-')\n",
    "    plt.xlabel(c1)\n",
    "    plt.ylabel(c2)\n",
    "    plt.show()\n",
    "\n",
    "for c in numcolsexOH:\n",
    "    plot2numerical(df_temp, c2, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot categories vs 2 numerical - not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hour vs pump arrival time\n",
    "# for each category in BoroughName\n",
    "c2 = \"FirstPumpArriving_AttendanceTime\"\n",
    "c1 = \"weekday\"\n",
    "c3 = \"IncGeo_BoroughName\"\n",
    "print(onehotencodings)\n",
    "for c in onehotencodings[c3]:\n",
    "    #print(c)\n",
    "    df_temp2 = df_temp[df_temp[c].apply(lambda x: x == 1)]\n",
    "    plot2numerical(df_temp2, c1, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do a random forest to see what affects dependent var "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mse(pred, act):\n",
    "    diff = np.mean(np.power(pred-act, 2))\n",
    "    return diff\n",
    "\n",
    "def oneRF(df, c_xs, c_y):\n",
    "    #print(c_xs, c_y)\n",
    "    x1 = df[c_xs].values\n",
    "    y1 = df[c_y].values.reshape(-1, 1)\n",
    "    #print(x1)\n",
    "    #print(y1)\n",
    "    #normalise the data\n",
    "    scaler = StandardScaler()\n",
    "    x1 = scaler.fit_transform(x1)\n",
    "    #x2 = scaler.transform(x2)\n",
    "    sv = RandomForestRegressor()\n",
    "    \n",
    "    estimators = []\n",
    "    estimators.append((\"RF\", sv))\n",
    "    model = Pipeline(estimators)\n",
    "    param_rf = {\"RF__max_depth\": [3],\n",
    "              #\"RF__max_features\": [1, 3],\n",
    "              \"RF__min_samples_split\": [ 3],\n",
    "              \"RF__min_samples_leaf\": [3],\n",
    "              \"RF__bootstrap\": [True, False],\n",
    "               }\n",
    "    \n",
    "    cv_splits = 2\n",
    "   \n",
    "   \n",
    "    svm = GridSearchCV(model, cv=cv_splits, param_grid=param_rf)\n",
    "    svm.fit(x1, y1)\n",
    "    y1_pred = svm.predict(x1)\n",
    "    s_in = mse(y1_pred, np.squeeze(y1))\n",
    "    #s_out = svm.score(x2, y2)\n",
    "    return s_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c2 = \"FirstPumpArriving_AttendanceTime\"\n",
    "#finding the best predictor of c2\n",
    "\n",
    "iteration = 0\n",
    "colsleft = numcols\n",
    "colsincluded = []\n",
    "total_mse = []\n",
    "iteration_list = [];\n",
    "#picker doesn't quite seem to be doing as good a job as all columns\n",
    "while(iteration < 160):\n",
    "    res_list = []\n",
    "    if(len(colsleft) == 0):\n",
    "        break\n",
    "    print(\"iteration\", iteration)\n",
    "    print(\"colsleft\", len(colsleft))\n",
    "    acc = 0;\n",
    "    for c in colsleft:\n",
    "        if(not c == c2):\n",
    "            temp_cols = colsincluded + [c]\n",
    "            #print(temp_cols)\n",
    "            s_in = oneRF(df, temp_cols, c2)\n",
    "            res_list.append({\"col\": c, \"s_in\": s_in})\n",
    "            acc +=1\n",
    "            print(\"(\"+str(acc)+\")\", s_in, end=\"\")\n",
    "\n",
    "    res = pd.DataFrame(res_list)\n",
    "    res = res.sort_values([\"s_in\"])\n",
    "    #res = res.reindex(range(0, len(res)))\n",
    "    print(res.head())\n",
    "    #---------------------------------------\n",
    "    # select best k columns\n",
    "    #---------------------------------------\n",
    "    for i in range(20):\n",
    "        if(len(res) > 0):\n",
    "            bestcol = res[\"col\"].iloc[0]\n",
    "            print(\"BEST COLUMN \", bestcol)\n",
    "            res = res.drop(res.index[0])\n",
    "            colsincluded.append(bestcol)\n",
    "    \n",
    "    s_cur = oneRF(df, colsincluded, c2)\n",
    "    iteration_list.append(iteration)\n",
    "    total_mse.append(s_cur)\n",
    "    \n",
    "    colsleft = res[\"col\"].values\n",
    "    iteration+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Fit benchmark of all columns\")\n",
    "#calc with all columns in it\n",
    "numcolsexc2 = list(set(numcols)- set([c2]))\n",
    "s_in_max = oneRF(df, numcolsexc2, c2)\n",
    "horiz_line_data = [iteration_list[0], iteration_list[-1]]\n",
    "plt.plot( horiz_line_data,[s_in_max, s_in_max], 'r') \n",
    "\n",
    "plt.plot(iteration_list, total_mse)\n",
    "print(len(numcols), s_in_max)\n",
    "print(len(colsincluded), total_mse[-1])\n",
    "plt.show()\n",
    "\n",
    "#might not use this function\n",
    "def dorandtes():    \n",
    "    randtests = 10\n",
    "    randdf = pd.DataFrame(np.random.randint(0,100,size=(df.shape[0], randtests)))\n",
    "    randdf[c2] = df[c2]\n",
    "    print(randdf.head())\n",
    "    randlist = []\n",
    "    for c in randdf.columns:\n",
    "        s_in = oneRF(randdf, [c], c2)\n",
    "        randlist.append(s_in)\n",
    "\n",
    "    mean = np.mean(randlist)\n",
    "    std = np.std(randlist)\n",
    "    up_limit = mean + 2* std\n",
    "    low_limit = mean - 2* std\n",
    "    res_list.append({\"col\": \"up_rand\", \"s_in\": up_limit})\n",
    "    res_list.append({\"col\": \"mean_rand\", \"s_in\": mean})\n",
    "    res_list.append({\"col\": \"low_rand\", \"s_in\": low_limit})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation\n",
    "-------\n",
    "1. General thoughts on the dataset\n",
    "2. Specific question\n",
    "* Is BLM charging the right amount vs man hours?\n",
    "3. What data do we have to answer that question?\n",
    "4. What data we use? high correlation\n",
    "5. LR of man hours vs charge\n",
    "6 Look at LR with groupings of different case types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(numcols), s_in_max)\n",
    "print(len(colsincluded), total_mse[-1])\n",
    "s_in_max = oneRF(df, numcols, c2)\n",
    "horiz_line_data = [iteration_list[0], iteration_list[-1]]\n",
    "plt.plot( horiz_line_data,[s_in_max, s_in_max], 'r') \n",
    "\n",
    "plt.plot(iteration_list, total_mse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
